{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T04:37:06.748747Z",
     "start_time": "2021-01-13T04:37:05.701793Z"
    }
   },
   "outputs": [],
   "source": [
    "# 抓取嵌套网页的title/对跳转页面进行解密\n",
    "from urllib import parse\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import requests\n",
    "import urllib.request \n",
    "import urllib.error\n",
    "from requests.exceptions import ReadTimeout,HTTPError,RequestException\n",
    "\n",
    "\n",
    "from html.parser import HTMLParser\n",
    "def decodeHtml(input):\n",
    "    h = HTMLParser()\n",
    "    s = h.unescape(input)\n",
    "    return s\n",
    "\n",
    "def get_formula(URL_HASH):\n",
    "    #URL加密公式部分\n",
    "    pattern_formula=re.compile(\"a\\[\\d\\]=\\\"([a-zA-Z0-9\\/:\\?=.]+)\\\";\")\n",
    "    formula=re.findall(pattern_formula,URL_HASH)\n",
    "    return(formula)\n",
    "\n",
    "def get_rp_list_new(URL_HASH):\n",
    "    #URL加密因子部分（源字符串筛选分割）\n",
    "    pattern_factor=re.compile(\",\\'([a-zA-Z0-9\\|]+)\\'.split\\(\\'\\|\\'\\),\")\n",
    "    result2=re.findall(pattern_factor,URL_HASH)\n",
    "    result2_list=result2[-1].split('|')\n",
    "    #print(result2_list)\n",
    "    \n",
    "    #URL加密因子部分（制作0-9，a-z，A-Z）\n",
    "    rp_list=[]\n",
    "    j=48\n",
    "    while j<=122:\n",
    "        if  j>=48 and j<=57:\n",
    "            rp_list.append(chr(j))\n",
    "            j+=1\n",
    "        elif j>=65 and j<=90:\n",
    "            rp_list.append(chr(j).lower())\n",
    "            j+=1\n",
    "        elif j>=97 and j<=122:\n",
    "            rp_list.append(chr(j).upper())\n",
    "            j+=1\n",
    "        else:\n",
    "            j+=1\n",
    "\n",
    "    #URL加密因子部分（并入对应值）\n",
    "    k=0\n",
    "    for i in result2_list:\n",
    "        if k<len(result2_list):\n",
    "            rp_list[k]=[rp_list[k],i]\n",
    "            k+=1\n",
    "\n",
    "    #URL加密因子部分（删除多余的）        \n",
    "    m=len(result2_list)\n",
    "    while m<len(rp_list):\n",
    "        del(rp_list[m])\n",
    "\n",
    "    #URL加密因子部分（删除空的）\n",
    "    rp_list_new=[]\n",
    "    for n in rp_list:\n",
    "        if len(n[1])>0:\n",
    "            rp_list_new.append(n)\n",
    "        else:\n",
    "            pass\n",
    "    return(rp_list_new)\n",
    "\n",
    "def formula_unescape(url_orig,rp_list_new):\n",
    "    address=[]\n",
    "    for i in url_orig:\n",
    "        for [u,v] in rp_list_new:\n",
    "            if u==i:\n",
    "                i=i.replace(u,v)\n",
    "                break\n",
    "            else:\n",
    "                pass\n",
    "        address.append(i)\n",
    "    return(''.join(address))\n",
    "\n",
    "def hashurl_unescape(url,URL_HASH):\n",
    "    formula=get_formula(URL_HASH)\n",
    "    rp_list_new=get_rp_list_new(URL_HASH)\n",
    "    html2=[]\n",
    "    for i in formula:\n",
    "        html2.append([url,''.join(formula_unescape(i,rp_list_new))])\n",
    "    return(html2)\n",
    "\n",
    "\n",
    "\n",
    "def getHtml(url):\n",
    "    req = requests.get(url=url)\n",
    "    req.encoding = None  \n",
    "    soup = BeautifulSoup(req.text,'lxml')\n",
    "    #html33=[]\n",
    "    for a in soup.find_all('script'):\n",
    "        html1=str(a.text)\n",
    "        #html2=hashurl_unescape(url,html1)\n",
    "        #html33=html33+html2\n",
    "    return (html1)\n",
    "\n",
    "\n",
    "def getSkipWeb1(url):\n",
    "    req = requests.get(url=url)\n",
    "    req.encoding = None  \n",
    "    soup = BeautifulSoup(req.text,'lxml')\n",
    "    a=soup.find_all('meta')\n",
    "    pattern=re.compile(\"url=(http:\\/\\/[0-9a-zA-Z\\/:-\\?\\.]+)\")\n",
    "    output_list=re.findall(pattern,str(a))\n",
    "    return(output_list)\n",
    "\n",
    "def getSkipedWeb1(url):\n",
    "    req = requests.get(url=url)\n",
    "    req.encoding = None  \n",
    "    soup = BeautifulSoup(req.text,'lxml')\n",
    "    a=soup.find_all('script')\n",
    "    pattern=re.compile(\"\\\"(http:\\/\\/[0-9a-zA-Z\\/:-\\?\\.]+)\\\";\")\n",
    "    output_list=re.findall(pattern,str(a))\n",
    "    return(output_list)\n",
    "\n",
    "\n",
    "from html.parser import HTMLParser\n",
    "def decodeHtml(input):\n",
    "    h = HTMLParser()\n",
    "    s = h.unescape(input)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-13T08:09:53.122Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-5-c84e2cbbca67>, line 71)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  File \u001B[1;32m\"<ipython-input-5-c84e2cbbca67>\"\u001B[1;36m, line \u001B[1;32m71\u001B[0m\n\u001B[1;33m    try:\u001B[0m\n\u001B[1;37m    ^\u001B[0m\n\u001B[1;31mIndentationError\u001B[0m\u001B[1;31m:\u001B[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# 判断原始网页是否能打开，增加原始网页的模拟关闭，判断链接网页是否能打开，对title部分是跳转的网页进行处理\n",
    "# 调用chrome浏览器并后台运行\n",
    "#option=webdriver.ChromeOptions()\n",
    "#option.add_argument('headless')\n",
    "#driver = webdriver.Chrome(options=option)\n",
    "\n",
    "#url_origin=['http://www.cqfmlkj.com/','http://www.weidicar.com/bzh.php','http://www.gjhuali.com/',\n",
    "#    'http://www.hbltfm.com/index.html','http://www.mmzaojiao.com/','http://www.milaile.com/',\n",
    "#     'http://www.taoyiezu.com/','http://www.czcin.com/',' https://www.gdyiyue.com/',\n",
    "#    'https://www.szhouai.com/','http://www.hybet.net/#index','http://www.supaiw.com/',\n",
    "#    'lt127.com','yzc265.com','www.lt2228.com','www.lt076.com','https://pts227.com',\n",
    "#    'https://www.wofacai888.com/sc','www.itb999.com','www.vic308.com','js8281.com',\n",
    "#    '493.com','www.buyu9999.com','28836.com','letou.com','dhy1119.com','https://www.fulibet88.com/',\n",
    "#    'https://www.toe9570.com/#/register?agentCode=1Ef0P02l','https://888.qq8p.top/',\n",
    "#    'http://www.lhtpxm.icu/','http://www.isheng360.com/db.php','http://www.38331155.com:8888/?a=10295124',\n",
    "#    'http://www.44009908.com/','http://www.rdnxqvk.cn/',\n",
    "#    'http://www.jitaba.cn/applkX/Kw/','http://www.qt211.com/mip/cO26CH.html',\n",
    "#    'http://51fa51.com/','http://103.194.186.43:9966/','http://www.ufo-1.cn/b6xuf8/7197084833.html',\n",
    "#    'https://cszx1.com/','http://bcfc5880.vip','321365.com','dsndsn168.com',\n",
    "#     '53618a.com','921613.com','1889tyc.com','077qm.com','https://www.sk500.cn',\n",
    "#     '9192655.com','http://m.yihaocai55.com/home','7606.com','www.2348.cc',\n",
    "#     'b2b.louzhu520.com/','https://www.jitatang.com/paogjcym.html',' http://home.slieny.com/vipbkll/',\n",
    "#     'www.wtobag.com/hot3n6/','https://www.chalook.net/vipqchh/','http://md.itlun.cn/hot-CYUae9Hl.html',\n",
    "#     'http://www.cc222.com/FaUP8FAjGB.xml','http://www.txahz.com/dst7G3SlH.html']\n",
    "\n",
    "url_origin=['http://www.weidicar.com','http://www.gjhuali.com/',\n",
    "    'http://www.mmzaojiao.com/','http://www.milaile.com/',\n",
    "    'http://www.taoyiezu.com/','http://www.czcin.com/',\n",
    "    'http://www.hybet.net/#index',\n",
    "    'lt127.com','yzc265.com','www.lt2228.com','www.lt076.com','https://pts227.com',\n",
    "    'www.itb999.com','www.vic308.com','js8281.com',\n",
    "    '493.com','www.buyu9999.com','28836.com','letou.com','dhy1119.com','https://www.fulibet88.com/',\n",
    "    'https://www.toe9570.com/#/register?agentCode=1Ef0P02l','https://888.qq8p.top/',\n",
    "    'http://www.isheng360.com/db.php','http://www.38331155.com:8888/?a=10295124',\n",
    "    'http://www.44009908.com/','http://www.rdnxqvk.cn/',\n",
    "    'http://www.jitaba.cn/applkX/Kw/','http://www.qt211.com/mip/cO26CH.html',\n",
    "    'http://51fa51.com/','http://103.194.186.43:9966/','http://www.ufo-1.cn/b6xuf8/7197084833.html',\n",
    "    'https://cszx1.com/','http://bcfc5880.vip','321365.com','dsndsn168.com',\n",
    "     '53618a.com','921613.com','1889tyc.com','077qm.com','https://www.sk500.cn',\n",
    "     '9192655.com','http://m.yihaocai55.com/home','7606.com','www.2348.cc',\n",
    "     'b2b.louzhu520.com/','https://www.jitatang.com/paogjcym.html',' http://home.slieny.com/vipbkll/',\n",
    "     'www.wtobag.com/hot3n6/','https://www.chalook.net/vipqchh/','http://md.itlun.cn/hot-CYUae9Hl.html',\n",
    "     'http://www.cc222.com/FaUP8FAjGB.xml','http://www.txahz.com/dst7G3SlH.html']\n",
    "\n",
    "data=[]\n",
    "for url_old in url_origin:\n",
    "    y=0\n",
    "    while y<5:\n",
    "        try:\n",
    "            option=webdriver.ChromeOptions()\n",
    "            option.add_argument('headless')\n",
    "            driver = webdriver.Chrome(options=option)\n",
    "            driver.get(url_old)\n",
    "            driver.set_page_load_timeout(15)\n",
    "            time.sleep(15)\n",
    "            #print(driver.current_url)\n",
    "            #print('-----------------------------------------------------------------------------------------------------------------')\n",
    "            iframe_name = driver.find_element_by_tag_name('iframe').get_attribute(\"src\")\n",
    "            iframe = driver.find_elements_by_tag_name('iframe')\n",
    "            #print(iframe_name)\n",
    "            driver.switch_to.frame(iframe[0])                          # 最重要的一步\n",
    "            soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "            driver.close()\n",
    "            for a in soup.find_all('a'):\n",
    "                    link = a['href']\n",
    "                    url = parse.urljoin(iframe_name, link)\n",
    "        except:\n",
    "            url = url_old            \n",
    "        print(url)   \n",
    "        print('------------------------------------------------------------------------------------------')\n",
    "            try:\n",
    "                req = requests.get(url)\n",
    "                req.encoding = None  \n",
    "        \n",
    "                soup = BeautifulSoup(decodeHtml(req.text),'lxml')\n",
    "                if (soup.find_all(\"title\")==[] or str(soup.find_all(\"title\"))=='[<title></title>]' or str(soup.find_all(\"title\"))=='[<title>正在进入</title>]') and soup.find_all(\"meta\")!=[] and getHtml(url).find('eval(function(p,a,c,k,e,d)')<=0:\n",
    "                    try:\n",
    "                        ScanSkipAddrList=getSkipWeb1(url)\n",
    "                        for ScanSkipAddr in ScanSkipAddrList:\n",
    "                            SkipedURL=getSkipedWeb1(ScanSkipAddr)\n",
    "                            for i in SkipedURL:\n",
    "                                data.extend([[url_old,url,i]])\n",
    "                    except:\n",
    "                        data.extend([[url_old,url,\"人工判断1\"]])\n",
    "                elif (soup.find_all(\"title\")==[] or str(soup.find_all(\"title\"))=='[<title></title>]' or str(soup.find_all(\"title\"))=='[<title>正在进入</title>]')  and getHtml(url).find('eval(function(p,a,c,k,e,d)')>0:\n",
    "                    try:\n",
    "                        html2=hashurl_unescape(url,getHtml(url))\n",
    "                        data.extend([[url_old,html2]])\n",
    "                    except:\n",
    "                        data.extend([[url_old,url,\"人工判断3\"]])\n",
    "                elif (soup.find_all(\"title\")!=[] and str(soup.find_all(\"title\"))!='[<title></title>]' and str(soup.find_all(\"title\"))!='[<title>正在进入</title>]'):\n",
    "                    try:\n",
    "                        data.extend([[url_old,url,''.join(str(soup.find_all('title')))]])\n",
    "                    except:\n",
    "                        data.extend([[url_old,url,\"人工判断4\"]])  \n",
    "                else :\n",
    "                    z=0\n",
    "                    while z<5:\n",
    "                        try:\n",
    "                            option=webdriver.ChromeOptions()\n",
    "                            option.add_argument('headless')\n",
    "                            driver = webdriver.Chrome(options=option)\n",
    "                            #driver = webdriver.Chrome()\n",
    "                            driver.get(url)\n",
    "                            driver.set_page_load_timeout(11)\n",
    "                            time.sleep(10)\n",
    "                            data.extend([[url_old,url,driver.current_url]])\n",
    "                            driver.close()\n",
    "                            z+=1\n",
    "                        except:\n",
    "                            data.extend([[url_old,url,\"人工判断5\"]])\n",
    "                            z+=1 \n",
    "                y+=1\n",
    "            except:\n",
    "                data.extend([[url_old,\"人工判断22\",'kong']])\n",
    "                y+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-12T08:49:04.592181Z",
     "start_time": "2021-01-12T08:49:04.572235Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "#print(data)\n",
    "data=numpy.array(data)\n",
    "df=pandas.DataFrame(data)\n",
    "df.head()\n",
    "#print(df1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-12T07:54:35.356062Z",
     "start_time": "2021-01-12T07:54:35.331134Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "#print(data)\n",
    "data=numpy.array(data)\n",
    "df=pandas.DataFrame(data)\n",
    "print(df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-12T07:58:21.336203Z",
     "start_time": "2021-01-12T07:58:21.327201Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-12T07:54:35.448205Z",
     "start_time": "2021-01-12T07:54:35.358059Z"
    }
   },
   "outputs": [],
   "source": [
    "df1=df.drop_duplicates(subset=[1,2], keep='first', inplace=False)\n",
    "df1.to_csv(\"a0112.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}